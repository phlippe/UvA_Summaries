\section{Deep RL in practice}
\textit{This section reviews the lecture slides 10 (last half).}
\begin{itemize}
	\item There are several things to keep in mind when performing experiments in RL in practice
	\item If we have a research questions that we want to investigate, we need to design experiments for which we have to answer the following questions:
\end{itemize}
\begin{description}
	\item[On which tasks?] We need to find environments which fit to the research question in mind. Things to consider are:
	\begin{itemize}
		\item Continuous control tasks lend themselves to actor critic methods
		\item Pixel-based task can show whether complex input data can be handled
		\item Highly complex tasks show whether a method scales with having lots of compute and training data available
		\item Toy examples can point out difference between methods, so it is often good to have both a toy example, and a more complex, practical one
	\end{itemize}
	\item[Which parameters and architectures to test?] RL have been shown to be very sensitive to the selection of hyperparameters. Hence, you should also spend similar tuning efforts on \underline{all} your experiments, including the baseline, to ensure a fair comparison.
	\item[Does a random seed affect my experiments?] Due to the high variance of the RL methods, we need to average all runs over a sufficient amount of seeds. Furthermore, if we perform a gridsearch, we should always keep the seeds fixed for all hyperparameter settings, but in the final test, use a different set of random seeds to prevent overfitting on seeds.
	\item[What to report?] Next to the mean and/or median performance, the spread of the result should be shown as well. Furthermore, it should represent what you want to show. For example, if we want to underline that a new method learns faster, we should show a plot over learning iterations instead of just the final performance.
\end{description}
